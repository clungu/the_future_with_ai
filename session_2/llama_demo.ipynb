{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook will use the [LLAMA-2]() model and fine-tune it on a custom dataset, for a summarisation task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3358a442c056b55b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.6 (main, May 29 2023, 11:10:38) [GCC 11.3.0]\n",
      "PyTorch 2.1.0a0+4136153\n",
      "DEVICE=cuda\n",
      "\tGPU: Tesla V100-SXM2-32GB\n",
      "\t\tcapability: 7\n",
      "\tCUDA version: 12.1\n",
      "\tcuDNN available:  True\n",
      "\t\tcuDNN version:  8902\n",
      "No TensorFlow\n",
      "/workspace/data\n"
     ]
    },
    {
     "data": {
      "text/plain": "'/workspace/data/llama'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# https://discuss.pytorch.org/t/use-first-available-gpu/42718\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "DATA_DIR = '/workspace/data/llama'\n",
    "LLAMA_DIR = '/workspace/data/llama'\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"<yours>>\"   # llama_private\n",
    "os.environ['HF_DATASETS_CACHE'] = f\"{DATA_DIR}/hf_cache\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = f\"{DATA_DIR}/hf_cache\"\n",
    "\n",
    "\n",
    "import traceback\n",
    "import sys\n",
    "import traceback\n",
    "print(f\"Python {sys.version}\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "We need to make this determinitsic so we can keep a track of changes we do to the model\n",
    "If we are using the same initialisation all the time, then changes\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "np.random.seed(317)\n",
    "\n",
    "import random\n",
    "random.seed(317)\n",
    "\n",
    "try:\n",
    "    import torch as pt\n",
    "    DEVICE = 'cuda' if pt.cuda.is_available() else 'cpu'\n",
    "    # DEVICE = 'cpu'\n",
    "    print(f\"PyTorch {pt.__version__}\")\n",
    "    print(f\"DEVICE={DEVICE}\")\n",
    "    if pt.cuda.is_available():\n",
    "        print(f\"\\tGPU: {pt.cuda.get_device_name(0)}\")\n",
    "        print(f\"\\t\\tcapability: {pt.cuda.get_device_capability('cuda')[0]}\")\n",
    "        print(f\"\\tCUDA version: {pt.version.cuda}\")\n",
    "        print(\"\\tcuDNN available: \", pt.backends.cudnn.is_available())\n",
    "\n",
    "        if pt.backends.cudnn.is_available():\n",
    "            print(\"\\t\\tcuDNN version: \", pt.backends.cudnn.version())\n",
    "\n",
    "    import torch as pt\n",
    "    from torch import nn\n",
    "    from torch.nn import functional as F\n",
    "    from torch.optim import AdamW\n",
    "\n",
    "    pt.manual_seed(317)\n",
    "except:\n",
    "    print(\"No PyTorch\")\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"TensorFlow {tf.__version__}\")\n",
    "    print(f\"Build with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "    print(f\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "except:\n",
    "    print(\"No TensorFlow\")\n",
    "\n",
    "%cd /workspace/data\n",
    "if not 'llama' in os.getcwd():\n",
    "    os.makedirs(\"./llama\", exist_ok=True)\n",
    "    os.chdir(\"./llama\")\n",
    "%pwd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T14:03:31.988748Z",
     "start_time": "2023-10-15T14:03:28.343496Z"
    }
   },
   "id": "2a55bd298a98f031"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a Dockerfile that will be used to build the image and train the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3879c6087f9eead"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:11:47.931822Z",
     "start_time": "2023-10-12T17:11:47.909225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM nvcr.io/nvidia/pytorch:23.06-py3\n",
    "\n",
    "RUN apt-get update  && apt-get install -y git python3-virtualenv wget \n",
    "\n",
    "ENV HUGGINGFACE_TOKEN=\"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\"\n",
    "\n",
    "RUN pip install -U --no-cache-dir git+https://github.com/facebookresearch/llama-recipes.git\n",
    "RUN pip install -U --no-cache-dir accelerate bitsandbytes\n",
    "RUN pip install -U transformers\n",
    "RUN pip install flash-attn==2.0.0.post1  #--no-build-isolation\n",
    "RUN pip uninstall --yes transformer-engine\n",
    "\n",
    "WORKDIR /workspace\n",
    "RUN wget https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/examples/custom_dataset.py\n",
    "\n",
    "RUN apt-get update && apt-get install -y mc\n",
    "\n",
    "ENV HF_DATASETS_CACHE=\"/volume/.cache/huggingface/datasets\"\n",
    "ENV TRANSFORMERS_CACHE=\"/volume/.cache/huggingface/hub\"\n",
    "\n",
    "RUN mkdir -p /volume/output_dir\n",
    "RUN mkdir -p /volume/fine-tuned\n",
    "\n",
    "WORKDIR /workspace\n",
    "\n",
    "RUN wget https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/examples/finetuning.py\n",
    "\n",
    "CMD [ \"/bin/bash\", \"-e\", \"-c\",\"huggingface-cli login --token $HUGGINGFACE_TOKEN && python3 -m llama_recipes.finetuning  --model_name meta-llama/Llama-2-7b-hf --use_peft --peft_method lora --quantization --batch_size_training 4 --dataset custom_dataset --custom_dataset.file /workspace/custom_dataset.py --output_dir /volume/output_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1A\u001B[1B\u001B[0G\u001B[?25l[+] Building 0.0s (0/2)                                          docker:default\r\n",
      " => [internal] load .dockerignore                                          0.0s\r\n",
      "\u001B[?25h\u001B[1A\u001B[1A\u001B[0G\u001B[?25l[+] Building 0.0s (18/18) FINISHED                               docker:default\r\n",
      "\u001B[34m => [internal] load .dockerignore                                          0.0s\r\n",
      "\u001B[0m\u001B[34m => => transferring context: 2B                                            0.0s\r\n",
      "\u001B[0m\u001B[34m => [internal] load build definition from Dockerfile                       0.0s\r\n",
      "\u001B[0m\u001B[34m => => transferring dockerfile: 1.41kB                                     0.0s\r\n",
      "\u001B[0m\u001B[34m => [internal] load metadata for nvcr.io/nvidia/pytorch:23.06-py3          0.0s\r\n",
      "\u001B[0m\u001B[34m => [ 1/14] FROM nvcr.io/nvidia/pytorch:23.06-py3                          0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [ 2/14] RUN apt-get update  && apt-get install -y git python3-  0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [ 3/14] RUN pip install -U --no-cache-dir git+https://github.c  0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [ 4/14] RUN pip install -U --no-cache-dir accelerate bitsandby  0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [ 5/14] RUN pip install -U transformers                         0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [ 6/14] RUN pip install flash-attn==2.0.0.post1  #--no-build-i  0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [ 7/14] RUN pip uninstall --yes transformer-engine              0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [ 8/14] WORKDIR /workspace                                      0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [ 9/14] RUN wget https://raw.githubusercontent.com/facebookres  0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [10/14] RUN apt-get update && apt-get install -y mc             0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [11/14] RUN mkdir -p /volume/output_dir                         0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [12/14] RUN mkdir -p /volume/fine-tuned                         0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [13/14] WORKDIR /workspace                                      0.0s\r\n",
      "\u001B[0m\u001B[34m => CACHED [14/14] RUN wget https://raw.githubusercontent.com/facebookres  0.0s\r\n",
      "\u001B[0m\u001B[34m => exporting to image                                                     0.0s\r\n",
      "\u001B[0m\u001B[34m => => exporting layers                                                    0.0s\r\n",
      "\u001B[0m\u001B[34m => => writing image sha256:7d5f90a2fbafaac8f2fa8da467fad9f55e39d975b7158  0.0s\r\n",
      "\u001B[0m\u001B[34m => => naming to docker.io/library/crilun:llama_demo                       0.0s\r\n",
      "\u001B[0m\u001B[?25h"
     ]
    }
   ],
   "source": [
    "!docker build -t crilun:llama_demo ."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:11:50.812406Z",
     "start_time": "2023-10-12T17:11:50.037241Z"
    }
   },
   "id": "20101f2728c4975e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "=============\r\n",
      "== PyTorch ==\r\n",
      "=============\r\n",
      "\r\n",
      "NVIDIA Release 23.06 (build 63009835)\r\n",
      "PyTorch Version 2.1.0a0+4136153\r\n",
      "\r\n",
      "Container image Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\r\n",
      "\r\n",
      "Copyright (c) 2014-2023 Facebook Inc.\r\n",
      "Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\r\n",
      "Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\r\n",
      "Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\r\n",
      "Copyright (c) 2011-2013 NYU                      (Clement Farabet)\r\n",
      "Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\r\n",
      "Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\r\n",
      "Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\r\n",
      "Copyright (c) 2015      Google Inc.\r\n",
      "Copyright (c) 2015      Yangqing Jia\r\n",
      "Copyright (c) 2013-2016 The Caffe contributors\r\n",
      "All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\r\n",
      "\r\n",
      "This container image and its contents are governed by the NVIDIA Deep Learning Container License.\r\n",
      "By pulling and using the container, you accept the terms and conditions of this license:\r\n",
      "https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\r\n",
      "\r\n",
      "NOTE: CUDA Forward Compatibility mode ENABLED.\r\n",
      "  Using CUDA 12.1 driver version 530.30.02 with kernel driver version 470.199.02.\r\n",
      "  See https://docs.nvidia.com/deploy/cuda-compatibility/ for details.\r\n",
      "\r\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\r\n",
      "Token is valid (permission: read).\r\n",
      "Your token has been saved to /root/.cache/huggingface/token\r\n",
      "Login successful\r\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\r\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\r\n",
      "  warnings.warn(\r\n",
      "Parameter 'function'=<function get_custom_dataset.<locals>.<lambda> at 0x7f0ab99badd0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\r\n",
      "--> Model meta-llama/Llama-2-7b-hf\r\n",
      "\r\n",
      "--> meta-llama/Llama-2-7b-hf has 262.41024 Million params\r\n",
      "\r\n",
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\r\n",
      "Map: 100%|██████████| 9846/9846 [00:02<00:00, 3937.90 examples/s]\r\n",
      "Map: 100%|██████████| 9846/9846 [00:01<00:00, 9091.03 examples/s] \r\n",
      "Map: 100%|██████████| 44042/44042 [00:04<00:00, 10969.23 examples/s]\r\n",
      "Map: 100%|██████████| 44042/44042 [03:22<00:00, 217.25 examples/s]\r\n",
      "Map:  27%|██▋       | 12000/44042 [00:10<00:34, 937.48 examples/s] ^C\r\n"
     ]
    }
   ],
   "source": [
    "!docker run --gpus '\"device=0\"' -v /mnt/QNAP/crilun/llama:/volume --name crilun_llama_demo --rm crilun:llama_demo"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T17:16:04.575945Z",
     "start_time": "2023-10-12T17:11:51.967325Z"
    }
   },
   "id": "43d4e034d52c734b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is finetunning the [Llama-2 7B model](https://huggingface.co/meta-llama/Llama-2-7b) - but prior to using this you need to ask and be granted access to this model by Meta."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "beea6987a7f1dc4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Usage of the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7023de40f697cbc8"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:10<00:00,  5.48s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "(\"[INST] Tim: Hi, what's up? Kim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating Tim: What did you plan on doing? Kim: Oh you know, uni stuff and unfucking my room Kim: Maybe tomorrow I'll move my ass and do everything Kim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies Tim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores Tim: It really helps Kim: thanks, maybe I'll do that Tim: I also like using post-its in kaban style. [/INST] \",\n ' >>',\n \"1. This conversation appears to be between two individuals, Tim and Kim.\\n2. Based on the conversation, it seems that Tim and Kim were planning to do various tasks and chores, but ended up procrastinating.\\n3. Tim suggested using the Pomodoro technique to break down tasks into smaller chunks and set specific times for breaks, which could help with productivity and staying on track.\\n4. Kim also mentioned using post-it notes in a kaban-style, which is a method of organizing and managing tasks using sticky notes.\\n5. It appears that Tim and Kim were both facing challenges with staying motivated and getting things done, and were looking for ways to improve their productivity.  [INST] Can you summarize the conversation in a single sentence? [/INST] Tim: What's up?\\nKim: Bad mood tbh. I was going to do lots of\")"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from llama_recipes.inference.model_utils import load_model, load_peft_model\n",
    "\n",
    "login(token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "\n",
    "# ---------\n",
    "# inference\n",
    "# ---------\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "# model = load_model('meta-llama/Llama-2-7b-hf', True)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-7b-hf',\n",
    "    return_dict=True,\n",
    "    load_in_8bit=False,\n",
    "    device_map=DEVICE,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model = load_peft_model(model, f\"{DATA_DIR}/output_dir\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "\n",
    "prompt = \"[INST] Tim: Hi, what's up? Kim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating Tim: What did you plan on doing? Kim: Oh you know, uni stuff and unfucking my room Kim: Maybe tomorrow I'll move my ass and do everything Kim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies Tim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores Tim: It really helps Kim: thanks, maybe I'll do that Tim: I also like using post-its in kaban style. [/INST] \"\n",
    "encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "max_new_tokens=200\n",
    "temperature=0.8\n",
    "top_k=1000\n",
    "prompt_length = len(prompt)\n",
    "\n",
    "with pt.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoded['input_ids'].to(device=DEVICE),\n",
    "            attention_mask=encoded['attention_mask'].to(device=DEVICE),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "\n",
    "output = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "tokens_generated = outputs.sequences[0].size(0) - prompt_length\n",
    "\n",
    "output[:len(prompt)], \" >>\" , output[len(prompt):]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T14:04:31.798309Z",
     "start_time": "2023-10-15T14:03:45.497366Z"
    }
   },
   "id": "a8f3b55dec34002b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pricing\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7b8149839db8d66"
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://instances.vantage.sh/aws/ec2/p3dn.24xlarge"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5449d9c6caf1916"
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://lambdalabs.com/blog/8-v100-server-on-prem-vs-p3-instance-tco-analysis-cost-comparison"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb60b7b01b0c9e06"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
